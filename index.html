<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0"/>
  <title>MPI-Based Multi-Node Tensor Parallelism</title>
  <link rel="stylesheet" href="style.css" />
</head>
<body>
  <header>
    <h1>MPI-Based Multi-Node Tensor Parallelism</h1>
    <p class="team">Team: Ruiqi Yang, Zijia Liu</p>
  </header>

  <main>
    <section>
      <h2>Project Summary</h2>
      <p>
        We will implement a multi-node tensor parallelism and focus on communication optimization with MPI.
        By replacing built-in collective operations with optimized AllReduce and Scatter-Reduce algorithms, 
        we aim to gain fine-grained control over communication and explore strategies that overlap GPU 
        computation with inter-node data exchange. This allows us to evaluate the correctness, memory 
        efficiency, and performance of parallel attention mechanisms without requiring model training.
      </p>
    </section>

    <section>
      <h2>Background</h2>
      <p>
        In modern Transformer models, one of the biggest challenges during multi-node distributed 
        inference is the communication overhead, which slows down system performance. This is 
        especially true when tensor parallelism is used to split the model across multiple devices, 
        as the devices need to share and combine data. For example, different GPUs might store pieces 
        of the weights for a certain layer of the model, and they need to communicate to merge the 
        results at each computation step. This communication can cause delays and waste resources in 
        large-scale setups. This project aims to improve the way data is shared between devices during 
        Transformer inference. We’re developing custom methods, like AllReduce and Scatter-Reduce, based 
        on MPI, to replace the standard communication methods used in existing libraries (like 
        MPI_Allreduce). We plan to implement strategies like Ring AllReduce and Recursive Doubling and 
        explore ways to overlap communication with GPU computation. By doing this, we hope to reduce 
        the impact of communication delays. Our goal is to perform communication tasks, such as sending 
        and receiving data, at the same time as the computation, so that they don’t slow each other down.
      </p>
    </section>

    <section>
      <h2>Challenges</h2>
      <p>
        Attention mechanisms require global interaction between queries, keys, and values (three different matrices), which means that even when computation is distributed, nodes must frequently exchange data—creating potential communication bottlenecks. Unlike operations such as convolution, attention cannot be easily split into fully independent parts because each position in the sequence will depend on all others. 
      </p>
      <p>
        In addition, varying sequence lengths may lead to imbalanced workloads across processes, which reduces overall efficiency. To address this, we will explore optimization strategies to mitigate the resulting bottlenecks.
      </p>
      <p>
        We hope to gain deeper insight into the factors that impact communication bottlenecks and bandwidth usage, helping us build intuition for optimizing communication. At the same time, we want to enhance our understanding of tensor parallelism and explore more efficient model partitioning strategies to reduce the performance impact of communication bottlenecks like AllReduce during inference.
      </p>
    </section>

    <section>
      <h2>Goals</h2>
      <ul>
        <li>Implement Ring AllReduce and Recursive Doubling for tensor parallelism.</li>
        <li>Compare custom collectives with built-in MPI_Allreduce.</li>
        <li>Explore parallel sequence inference and dynamic load balancing.</li>
      </ul>
    </section>

    <section>
      <h2>Platform & Resources</h2>
      <p>
        We use GPU clusters with MPI support (GHC, PSC). Our experiments will be based on open-source models 
        such as Meta's LLaMA, focusing on inference. Code will be written in C++ or Python depending on the 
        framework used.
      </p>
    </section>

    <section>
      <h2>Schedule</h2>
      <ul>
        <li><strong>Mar 25–31</strong>: Research + environment setup</li>
        <li><strong>Apr 1–7</strong>: Baseline AllReduce tests</li>
        <li><strong>Apr 8–14</strong>: Implement + integrate Ring AllReduce</li>
        <li><strong>Apr 15</strong>: Midpoint milestone</li>
        <li><strong>Apr 15–21</strong>: Parallel sequences + load balancing</li>
        <li><strong>Apr 22–28</strong>: Final testing + report + poster</li>
      </ul>
    </section>

    <footer>
      <p>&copy; <span id="year"></span> Ruiqi Yang & Zijia Liu</p>
    </footer>
  </main>

  <script src="script.js"></script>
</body>
</html>
